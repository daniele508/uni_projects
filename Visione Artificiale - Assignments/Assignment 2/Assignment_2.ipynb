{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per produrre un sistema di riconoscimento facciale vengono utilizzate le librerie python ```OpenCV```, che permette di manipolare le immagini, ```numpy```, che permette di gestire i dati in forma vettoriale e matriciale, e ```os```, che permette di esplorare i file navigando nei vari percorsi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per riconoscere i volti presenti nelle immagini viene usato Viola Jones, che fa uso di alcune semplici caratteristiche del volto, chiamate Haar-like features, e vengono computatr in maniera veloce utilizzando le immagini integrali.\n",
    "\n",
    "Il metodo ```viola_jones_init``` inizializza il riconoscitore Viola Jones ritornando il classificatore già addestrato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viola_jones_init():\n",
    "    face_xml='haarcascade_frontalface_alt.xml'\n",
    "\n",
    "    face_cascade=cv2.CascadeClassifier()\n",
    "\n",
    "    if not face_cascade.load(face_xml):\n",
    "        print('--(!)Error loading face cascade')\n",
    "        exit(0)\n",
    "    \n",
    "    return face_cascade\n",
    "\n",
    "face_cascade=viola_jones_init()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per implementare un sistema chiuso di riconoscimento facciale basato sulle eigenfaces viene costruito prima uno spazio delle facce. Per poterlo costruire si parte da un dataset di 2000 immagini di volti.\n",
    "\n",
    "Il metodo ```initialize_dataset``` si occupa di di esplorare il dataset, convertire le immagini in scala di grigio, attraverso Viola Jones individuare il volto, riscalarlo in 64x64 pixel e vettorizzarlo.\n",
    "\n",
    "Verrà restituita una matrice le cui colonne corrispondono ad una immagine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_dataset(face_cascade, pathname):\n",
    "    images=[]\n",
    "\n",
    "    i=0\n",
    "    for dirpath, dirnames, filenames in  os.walk(pathname):\n",
    "        for filename in filenames:\n",
    "            \n",
    "            img=cv2.imread(os.path.join(dirpath, filename))\n",
    "            img_gray=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            faces=face_cascade.detectMultiScale(img_gray)\n",
    "\n",
    "            for x,y,w,h in faces:\n",
    "                face=img_gray[y:y+h, x:x+w]\n",
    "                face_resized=cv2.resize(face, (64,64))\n",
    "                images.append(face_resized.flatten())\n",
    "                i+=1\n",
    "    \n",
    "            if i>1999:\n",
    "                break\n",
    "\n",
    "        if i>1999:\n",
    "                break\n",
    "    \n",
    "    images=np.array(images).T\n",
    "    return images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una volta ottenuta la matrice di immagini vettorizzate, se ne calcola la media per trovare la mean face, utilizzata successivamente per costruire lo spazio delle facce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=initialize_dataset(face_cascade, 'images/dataset')\n",
    "\n",
    "mean=np.mean(images, axis=1)\n",
    "\n",
    "mean_face=mean.reshape((64,64)).astype(np.uint8)\n",
    "\n",
    "cv2.imshow(\"mean\", mean_face)\n",
    "cv2.imwrite(\"images/mean.png\", mean_face)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo spazio delle facce è utile perché si può ricorstruire qualsiasi faccia a partire dalla faccia media e aggiungendo delle immagini scalate.\n",
    "\n",
    "Queste immagini scalate vengono chiamate eigenfaces, e corrispondono a dei vettori ortonormali $u_{i}$, ricavati effettuando la Singular Value Decomposition della matrice di covarianza $C$, ricavata a sua volta dalla matrice delle immagini:\n",
    "\n",
    "![covarianza](images/covarianza.png)\n",
    "\n",
    "![eigenfaces](images/eigenfaces.png)\n",
    "\n",
    "Le eigenfaces $u_{i}$ saranno quindi i vettori colonna della matrice $U$.\n",
    "\n",
    "Il metodo ```compute_eigenfaces``` ricava prima la matrice di covarianza per poi eseguirne la SVD, e ritorna la matrice delle eigenfaces e il vettore che contiene gli autovalori ordinati in ordine decrescente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eigenfaces(images):\n",
    "    covar=np.cov(images)\n",
    "    U,S,V=np.linalg.svd(covar)\n",
    "\n",
    "    return U, S\n",
    "\n",
    "eigenfaces, S=compute_eigenfaces(images)\n",
    "\n",
    "for i in range(4):\n",
    "    eigenface=cv2.normalize(eigenfaces[:,i].reshape((64,64)), None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    cv2.imwrite(\"images/eigenface_\"+str(i+1)+\".png\", eigenface)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viene definito il metodo ```acquisizione_foto``` che permette, attraverso il tasto \"a\", di acquisire foto, e contemporanemanete, attraverso Viola Jones, rileva i volti presenti nelle immagini, disegnando un rettangolo attorno ad ognuno di loro, in modo da essere sicuri che nella foto scattata è presente un volto.\n",
    "\n",
    "E' utilizzato per acquisire le foto di 4 persone per poter costruire una galleria da utilizzare come training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquisizione_foto(face_cascade):\n",
    "    src = 0 # webcam\n",
    "    #src = 'rtsp://CV2023:Studente123@147.163.26.184:554/Streaming/Channels/101'\n",
    "    #src = 'rtsp://CV2023:Studente123@147.163.26.182:554/Streaming/Channels/101'\n",
    "\n",
    "    video = cv2.VideoCapture(src)\n",
    "\n",
    "    if not video.isOpened():\n",
    "        print('--(!)Error opening video capture')\n",
    "        exit(0)\n",
    "\n",
    "    key = ''\n",
    "    i=1\n",
    "\n",
    "    while key!=ord('q'):\n",
    "        ret, frame = video.read()\n",
    "\n",
    "        frame_gray=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces=face_cascade.detectMultiScale(frame_gray)\n",
    "\n",
    "        rect=frame.copy()\n",
    "    \n",
    "        for x,y,w,h in faces:\n",
    "            rect=cv2.rectangle(rect, (x,y), (x+w,y+h), (41,59,52), 4)\n",
    "            #blu oceano (74,51,29)\n",
    "            #verde bottiglia (41,59,52)\n",
    "            #giallo (0,216,255)\n",
    "\n",
    "        cv2.imshow(\"frame\", rect)\n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "    \n",
    "        if key==ord('a'):\n",
    "            string=\"images/image_\"+str(i)+\".png\"\n",
    "            cv2.imwrite(string, frame)\n",
    "            i+=1\n",
    "\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "\n",
    "acquisizione_foto(face_cascade)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il metodo ```acquisizione_video``` permette di acquisire video alla pressione del tasto \"a\" e di salvarlo premendo il tasto \"s\".\n",
    "\n",
    "E' utilizzato per poter acquisire un breve video in cui verranno etichettati tutti i volti rilevati, in modo da usarlo come validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquisizione_video():\n",
    "    src = 0 # webcam\n",
    "    #src = 'rtsp://CV2023:Studente123@147.163.26.184:554/Streaming/Channels/101'\n",
    "    #src = 'rtsp://CV2023:Studente123@147.163.26.182:554/Streaming/Channels/101'\n",
    "\n",
    "    video = cv2.VideoCapture(src)\n",
    "\n",
    "    video_FourCC = int(video.get(cv2.CAP_PROP_FOURCC))\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    H = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    W = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_size = (W, H)\n",
    "\n",
    "    if video is None or not video.isOpened():\n",
    "        print('--(!)Error opening video capture')\n",
    "        exit(0) \n",
    "\n",
    "    key = ''\n",
    "    out=None\n",
    "    i=1\n",
    "\n",
    "    while key!=ord('q'):\n",
    "\n",
    "        ret, frame = video.read()\n",
    "        cv2.imshow(\"frame\", frame)\n",
    "        key = cv2.waitKey(1)\n",
    "\n",
    "        if key==ord('a'):\n",
    "            out = cv2.VideoWriter('images/validate/validation_'+str(i)+'.mp4',  video_FourCC, fps, frame_size)\n",
    "            i+=1\n",
    "            if out is None or not out.isOpened():\n",
    "                print('--(!)Error opening video capture')\n",
    "                exit(0)\n",
    "            while key!=ord('s'):\n",
    "                cv2.imshow(\"frame\", frame)\n",
    "                out.write(frame)\n",
    "                ret, frame = video.read()\n",
    "                key= cv2.waitKey(1)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "    if out is not None:\n",
    "        out.release()\n",
    "\n",
    "acquisizione_video()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il metodo ```detect_face```, attraverso Viola Jones, rileva i volti presenti all'interno dei video, e ritorna una matrice di immagini vettorizzate e distribuite per colonne.\n",
    "\n",
    "Il metodo ```project```, invece, calcola i coefficenti $a$ attraverso la formula:\n",
    "\n",
    "![coefficenti](images/coefficenti.png)\n",
    "\n",
    "Questi coefficenti scalano le eigenfaces da utilizzare per poter ricostruire l'immagine di partenza. Sostanzialmente l'immagine di partenza viene proiettata all'interno dello spazio delle facce e ricostruita partendo dal centro dello spazio, ovvero la media, e aggiungendo i vettori scalati, quindi le eigenfaces moltiplicate ai coefficenti $a$. La formula dell'approssimazione dell'immagine è quindi:\n",
    "\n",
    "![approssimazione](images/approssimazione.png)\n",
    "\n",
    "Lo spazio delle facce è invece rappresentato:\n",
    "\n",
    "![spazio](images/face_space.png)\n",
    "\n",
    "Utilizzare i coefficenti $a$ per rappresentare le facce, e quindi proiettarle nello spazio, è molto utile perché la distanza tra due facce, cioè riconoscerne l'identità, può essere calcolata semplicemente calcolando la distanza L2 tra l'immagine proiettata e quelle presenti nello spazio:\n",
    "$$L_{2}=||\\textbf{a}-\\textbf{a}_{i}||_{2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(face_cascade):\n",
    "\n",
    "    images=[]\n",
    "\n",
    "    video=cv2.VideoCapture(\"images/validate/validation.mp4\")\n",
    "\n",
    "    if video is None or not video.isOpened(): \n",
    "        print('--(!)Error opening video capture')\n",
    "        exit(0)\n",
    "\n",
    "    ret, img = video.read()\n",
    "    i=0\n",
    "    while ret:\n",
    "\n",
    "        img_gray=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces=face_cascade.detectMultiScale(img_gray)\n",
    "\n",
    "        for x,y,w,h in faces:\n",
    "            \n",
    "            face=img_gray[y:y+h, x:x+w]\n",
    "\n",
    "            face_resized=cv2.resize(face, (64,64))\n",
    "\n",
    "            cv2.imwrite(\"images/validate/validate_\"+str(i)+\".png\", face_resized)\n",
    "            i+=1\n",
    "\n",
    "            images.append(face_resized.flatten())\n",
    "\n",
    "        ret, img = video.read()\n",
    "        \n",
    "    images=np.array(images).T\n",
    "    return images\n",
    "\n",
    "def project(n_components, gallery, eigenfaces):\n",
    "\n",
    "    n_images=gallery.shape[1]\n",
    "\n",
    "    a=np.zeros((n_images, n_components))\n",
    "\n",
    "    for i in range(n_images):\n",
    "        for j in range(n_components):\n",
    "            a[i,j]=np.dot(gallery[:,i]-mean, eigenfaces[:,j])\n",
    "\n",
    "    return a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viene importato il K Neighbors Classifier, ovvero un classificatore che si basa sulla distanza da un numero k di vicini, dove k è un iperparametro.\n",
    "Inoltre vengono scalati gli autovalori dividendo ognuno di loro per la somma totlae, in modo che la loro somma sia 1.\n",
    "Infine viene inizializzata la galleria (il training set), quindi viene istanziata la matrice che contiene le immagini vettorizzate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "S_scaled=S/np.sum(S)\n",
    "\n",
    "### INIZIALIZZAZIONE TRAINING SET ###\n",
    "gallery=initialize_dataset(face_cascade, 'images/gallery')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per addestrare il classificatore, deve essere associata un'echitetta ad ogni immagine, che corrisponde all'identità della faccia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LABEL TRAINING SET ###\n",
    "\n",
    "name1=np.tile(\"Alessandro\", 9)\n",
    "name2=np.tile(\"Antonio\", 13)\n",
    "name3=np.tile(\"Daniele\", 12)\n",
    "name4=np.tile(\"Rosario\", 15)\n",
    "classes=np.concatenate([name1,name2,name3,name4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come per il training set, viene inizializzata la matrice che contiene le immagini vettorizzate ricavate dall'analisi dei volti presenti nel video che funge da validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INIZIALIZZAZIONE VALIDATION SET ### \n",
    "\n",
    "validation=detect_face(face_cascade)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poiché il validation set può contenere degli errori, si istanzia una nuova matrice che contiene soltanto le immagini che ritraggono un volto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PULIZIA DATI ###\n",
    "noise=[23,27,31,35,36,40,41,45,46,50,51,53,57,58,59,60,64,65,66,70,71,75,79,83,84,85,90,91,95,96,101,103,107,112,117,120,125,130,131,136,137,142,148,151,156,162,164,168,170,171,175,180,184,189,190,195,196,197,200,201,204,209,210,215,216,221,222,223,225,228,231,232,236,237,238]\n",
    "n_validation=np.delete(validation, noise, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come per il training setm bisogna etichettare ogni volto del validation set per poter calcolare lo score del classificatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LABEL VALIDATION SET ###\n",
    "\n",
    "validate=[\"Antonio\", \"Alessandro\", \"Daniele\", \"Antonio\", \"Alessansdro\", \"Daniele\", \"Antonio\", \"Alessandro\", \"Daniele\", \"Antonio\", \"Alessandro\", \"Daniele\", \"Antonio\", \"Alessandro\", \"Daniele\", \"Antonio\", \"Alessandro\", \"Daniele\", \"Antonio\", \"Alessandro\", \"Daniele\", \"Antonio\", \"Alessandro\", \"Daniele\", \"Antonio\", \"Alessandro\", \"Daniele\", \"Antonio\", \"Alessandro\", \"Daniele\", \"Antonio\", \"Alessandro\", \"Daniele\", \"Antonio\", \"Alessandro\", \"Daniele\", \"Alessandro\", \"Antonio\", \"Daniele\", \"Alessandro\", \"Antonio\", \"Daniele\", \"Antonio\", \"Daniele\", \"Alessandro\", \"Antonio\", \"Daniele\", \"Alessandro\", \"Daniele\", \"Antonio\", \"Alessandro\", \"Daniele\", \"Antonio\", \"Alessandro\", \"Daniele\", \"Antonio\", \"Alessandro\", \"Daniele\", \"Antonio\", \"Alessandro\", \"Antonio\", \"Rosario\", \"Daniele\", \"Alessandro\", \"Daniele\", \"Antonio\", \"Alessandro\", \"Rosario\", \"Antonio\",\"Daniele\", \"Alessandro\", \"Rosario\", \"Daniele\", \"Antonio\", \"Alessandro\", \"Rosario\", \"Daniele\",\"Antonio\", \"Alessandro\", \"Rosario\", \"Antonio\", \"Daniele\", \"Alessandro\", \"Rosario\", \"Alessandro\", \"Rosario\", \"Daniele\", \"Antonio\", \"Alessandro\", \"Antonio\", \"Rosario\", \"Daniele\", \"Alessandro\", \"Rosario\", \"Daniele\", \"Antonio\", \"Alessandro\", \"Rosario\", \"Daniele\",\"Antonio\", \"Alessandro\", \"Antonio\",\"Rosario\", \"Daniele\", \"Rosario\", \"Alessandro\", \"Antonio\", \"Daniele\", \"Antonio\", \"Daniele\", \"Rosario\", \"Alessandro\", \"Alessandro\", \"Daniele\", \"Rosario\", \"Antonio\", \"Alessandro\", \"Antonio\", \"Daniele\", \"Rosario\", \"Rosario\", \"Alessandro\", \"Daniele\", \"Antonio\", \"Alessandro\", \"Daniele\", \"Rosario\", \"Antonio\", \"Alessandro\", \"Antonio\", \"Daniele\", \"Rosario\", \"Alessandro\", \"Daniele\", \"Antonio\", \"Rosario\", \"Antonio\", \"Daniele\", \"Rosario\", \"Alessandro\", \"Alessandro\", \"Antonio\", \"Daniele\", \"Rosario\", \"Alessandro\", \"Daniele\", \"Antonio\", \"Rosario\", \"Daniele\", \"Antonio\", \"Rosario\", \"Alessandro\", \"Alessandro\", \"Daniele\", \"Rosario\", \"Antonio\", \"Antonio\", \"Daniele\", \"Alessandro\", \"Antonio\", \"Daniele\", \"Antonio\", \"Alessandro\", \"Daniele\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si definisce il metodo ```min_index``` che restituisce l'indice della colonna della matrice delle eigenfaces affinché venga considerata una certa percentuale della varianza, passata per argomento al metodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_index(S, n):\n",
    "    sum=0\n",
    "    index=0\n",
    "    for i in range(S.shape[0]):\n",
    "        sum+=S[i]\n",
    "        if sum>=n:\n",
    "            index=i\n",
    "            break\n",
    "\n",
    "    return index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vengono effettuati 6 esperimenti in totale, testando il 95% e il 99,99% della varianza e impostando il classificatore con 1 3 e 5 vicini. Si calcola lo score per ogni esperimento in modo da poter scegliere gli iperparametri associati all'esperimento con lo score più alto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuratezza con il 95.0% della varianza e con K=1: 0.51829\n",
      "Accuratezza con il 95.0% della varianza e con K=3: 0.52439\n",
      "Accuratezza con il 95.0% della varianza e con K=5: 0.49390\n",
      "Accuratezza con il 99.99% della varianza e con K=1: 0.51220\n",
      "Accuratezza con il 99.99% della varianza e con K=3: 0.51220\n",
      "Accuratezza con il 99.99% della varianza e con K=5: 0.48780\n"
     ]
    }
   ],
   "source": [
    "for n in [0.95, 0.9999]:\n",
    "\n",
    "    index=min_index(S_scaled, n)\n",
    "\n",
    "    a_train= project(index, gallery, eigenfaces)\n",
    "\n",
    "    a_val=project(index, n_validation, eigenfaces)\n",
    "    \n",
    "    for k in [1,3,5]:\n",
    "        knn= KNN(n_neighbors=k)\n",
    "        knn.fit(a_train, classes)\n",
    "        predicted=knn.predict(a_val)\n",
    "        #print(predicted)\n",
    "        acc=knn.score(a_val, validate)\n",
    "        print(f\"Accuratezza con il {n*100}% della varianza e con K={k}: {acc:.5f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dai risultati riportati nella tabella, si evince che l'esperimento che ha avuto più successo è stato quello con $k=3$ vicini e considerando il 95% della varianza. Perciò si calcolano i coefficenti del training set e si addestra il classificatore.\n",
    "\n",
    "![tabella](images/tabella.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn=KNN(n_neighbors=3)\n",
    "n=min_index(S_scaled, 0.95)\n",
    "\n",
    "a_train=project(n, gallery, eigenfaces)\n",
    "\n",
    "knn.fit(a_train, classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infine, viene analizzato il test set, un video composto da circa 2000 frame, e viene prodotto un ulteriore video uguale al test set, in cui ogni volto presente viene etichettato disegnando un rettangolo attorno ad esso e scrivendo il nome dell'identità prevista dal classificatore.\n",
    "\n",
    "Ogni frame del video viene analizzato in questo modo: attraverso Viola Jones vengono identificati i volti in ogni frame, e per ogni volto vengono calcolati i coefficenti sulla quale il classificatore si basa per predire l'identità. Successivamente viene disegnato il rettangolo e aggiunta l'etichetta e, una volta esaminati tutti i volti presenti nel frame, si passa al frame successivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "video=cv2.VideoCapture(\"images/test.mp4\")\n",
    "\n",
    "video_FourCC = int(video.get(cv2.CAP_PROP_FOURCC))\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "H = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "W = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_size = (W, H)\n",
    "\n",
    "out = cv2.VideoWriter('images/test_l.mp4',  video_FourCC, fps, frame_size)\n",
    "\n",
    "if video is None or not video.isOpened() or out is None or not out.isOpened() :\n",
    "    print('--(!)Error opening video capture')\n",
    "    exit(0)\n",
    "\n",
    "ret, img = video.read()\n",
    "\n",
    "while ret:\n",
    "    \n",
    "    img_gray=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_cascade.detectMultiScale(img_gray)\n",
    "\n",
    "    for x,y,w,h in faces:\n",
    "            \n",
    "        face=img_gray[y:y+h, x:x+w]\n",
    "        face_resized=cv2.resize(face, (64,64)).flatten()\n",
    "\n",
    "        a=project(n, np.array([face_resized]).T, eigenfaces)\n",
    "\n",
    "        prediction=knn.predict(a)\n",
    "        #print(prediction)\n",
    "\n",
    "        img=cv2.rectangle(img, (x,y), (x+w,y+h), (74,51,29), 4)\n",
    "        img=cv2.putText(img, prediction[0], (x,y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,216,255), 2)\n",
    "    \n",
    "    out.write(img)\n",
    "    ret, img=video.read()\n",
    "\n",
    "video.release()\n",
    "out.release()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il metodo aggiuntivo ```rebuilt``` serve a ricostruire un'immagine e verificare la correttezza dei coefficenti calcolati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHECK FACES ##\n",
    "def rebuilt(a, eigenfaces):\n",
    "    sum=0\n",
    "    n_images,n_components=a.shape \n",
    "\n",
    "    for i in range(n_images):\n",
    "        for j in range(n_components):\n",
    "            sum+=a[i,j]*eigenfaces[:,j]\n",
    "\n",
    "        new=(mean+sum).reshape((64,64)).astype(np.uint8)\n",
    "        cv2.imshow(\"img\", new)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        sum=0    \n",
    "\n",
    "#a_val=project(4095, gallery, eigenfaces)\n",
    "rebuilt(a_train, eigenfaces)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
